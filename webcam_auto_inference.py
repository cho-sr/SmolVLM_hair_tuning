"""
SmolVLM Webcam Auto Inference (Fine-tuned)
3ì´ˆë§ˆë‹¤ ìë™ìœ¼ë¡œ inference ìˆ˜í–‰
Fine-tuned on Hair classification & description dataset
"""

import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForImageTextToText
from peft import PeftModel
import gradio as gr
import numpy as np
from datetime import datetime
import time

# =========================
# ì„¤ì •
# =========================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BASE_MODEL_ID = "HuggingFaceTB/SmolVLM-256M-Instruct"
FINETUNED_MODEL_PATH = "/root/crying_cv_vlm/checkpoint-105"  # âœ… ìµœì¢… í•™ìŠµëœ ëª¨ë¸ (checkpoint-105)
INFERENCE_INTERVAL = 3  # 3ì´ˆ ê°„ê²©

print(f"ğŸ”§ Device: {DEVICE}")
print(f"ğŸ“‚ Fine-tuned Model: {FINETUNED_MODEL_PATH}")
print("Loading model...")

# =========================
# ëª¨ë¸ ë¡œë“œ (Fine-tuned LoRA)
# =========================
from transformers import AutoModelForImageTextToText
from peft import PeftModel

print("1ï¸âƒ£ Loading base model...")
model = AutoModelForImageTextToText.from_pretrained(
    BASE_MODEL_ID,
    dtype=torch.bfloat16 if DEVICE == "cuda" else torch.float32,
    device_map="auto",
    attn_implementation="eager"
)

print("2ï¸âƒ£ Loading fine-tuned adapter...")
model = PeftModel.from_pretrained(
    model,
    FINETUNED_MODEL_PATH,
    device_map="auto"
)

print("3ï¸âƒ£ Merging adapter...")
model = model.merge_and_unload()
model.eval()

print("4ï¸âƒ£ Loading processor...")
processor = AutoProcessor.from_pretrained(FINETUNED_MODEL_PATH)

print("âœ… Model loaded!")
if torch.cuda.is_available():
    print(f"ğŸ’¾ VRAM: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")


def inference(image, question):
    """ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì„ ë°›ì•„ inference ìˆ˜í–‰"""
    
    if image is None:
        return "âš ï¸ ì›¹ìº ì—ì„œ ì´ë¯¸ì§€ë¥¼ ìº¡ì²˜í•´ì£¼ì„¸ìš”.", "ëŒ€ê¸° ì¤‘"
    
    if not question or question.strip() == "":
        question = "Describe this image in detail."
    
    try:
        # Convert to PIL Image
        if isinstance(image, np.ndarray):
            image = Image.fromarray(image).convert('RGB')
        elif not isinstance(image, Image.Image):
            return "âŒ ì˜ëª»ëœ ì´ë¯¸ì§€ í˜•ì‹", "ì—ëŸ¬"
        elif image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Prepare messages
        messages = [{
            "role": "user",
            "content": [{"type": "image"}, {"type": "text", "text": question}]
        }]
        
        # Process
        prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
        inputs = processor(text=prompt, images=[image], return_tensors="pt").to(DEVICE)
        
        # ì…ë ¥ ê¸¸ì´ ì €ì¥
        input_len = inputs["input_ids"].shape[-1]
        
        # Generate
        with torch.inference_mode():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                top_p=0.9
            )
        
        # Decode (ìƒˆë¡œ ìƒì„±ëœ í† í°ë§Œ)
        generated_ids = generated_ids[0][input_len:]
        response = processor.decode(generated_ids, skip_special_tokens=True).strip()
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        status = f"âœ… {timestamp}"
        
        return response if response else "(ë¹ˆ ì‘ë‹µ)", status
    
    except Exception as e:
        import traceback
        error_msg = traceback.format_exc()
        return f"âŒ ì—ëŸ¬: {str(e)}\n\n{error_msg}", "ì—ëŸ¬ ë°œìƒ"


# =========================
# Gradio UI
# =========================
with gr.Blocks(title="SmolVLM Auto Inference") as demo:
    gr.Markdown("""
    # ğŸ¥ SmolVLM ì›¹ìº  ìë™ ì¶”ë¡  (Fine-tuned)
    
    **3ì´ˆë§ˆë‹¤ ìë™ìœ¼ë¡œ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤**
    
    ### ëª¨ë¸ ì •ë³´:
    - **Base Model**: HuggingFaceTB/SmolVLM-256M-Instruct
    - **Fine-tuned on**: Hair classification & description dataset
    - **Training**: 5 epochs, Final loss: 1.1350
    
    ### ì‚¬ìš© ë°©ë²•:
    1. ì›¹ìº  í—ˆìš© ë° ì´ë¯¸ì§€ ìº¡ì²˜
    2. ì§ˆë¬¸ ì…ë ¥
    3. "ğŸš€ ìë™ ì¶”ë¡  ì‹œì‘" ë²„íŠ¼ í´ë¦­
    4. 3ì´ˆë§ˆë‹¤ ìë™ìœ¼ë¡œ ì¶”ë¡ ë©ë‹ˆë‹¤
    5. "â¸ï¸ ì¤‘ì§€" ë²„íŠ¼ìœ¼ë¡œ ë©ˆì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            # ì›¹ìº  (streaming í™œì„±í™”)
            webcam = gr.Image(
                label="ğŸ“· ì›¹ìº ",
                type="numpy",
                sources=["webcam"],
                streaming=True,  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
                height=400
            )
            
            # ì§ˆë¬¸ ì…ë ¥
            question = gr.Textbox(
                label="ğŸ’¬ ì§ˆë¬¸",
                placeholder="ì´ë¯¸ì§€ì— ëŒ€í•´ ë¬¼ì–´ë³´ê³  ì‹¶ì€ ê²ƒì„ ì…ë ¥í•˜ì„¸ìš”",
                value="Classify the hair length in this image. Possible values: short, mid, long. Output only one word.",
                lines=3
            )
            
            with gr.Row():
                start_btn = gr.Button("ğŸš€ ìë™ ì¶”ë¡  ì‹œì‘", variant="primary", scale=2)
                stop_btn = gr.Button("â¸ï¸ ì¤‘ì§€", variant="stop", scale=1)
        
        with gr.Column(scale=1):
            # ì¶œë ¥
            output = gr.Textbox(
                label="ğŸ¤– ì‘ë‹µ",
                lines=15,
                max_lines=20
            )
            
            # ìƒíƒœ
            status = gr.Textbox(
                label="ğŸ“Š ìƒíƒœ",
                value="ëŒ€ê¸° ì¤‘",
                lines=1
            )
            
            # ìë™ ì¶”ë¡  ìƒíƒœ
            auto_status = gr.Textbox(
                label="ğŸ”„ ìë™ ì¶”ë¡  ìƒíƒœ",
                value="ë©ˆì¶¤",
                lines=1
            )
    
    # ì˜ˆì‹œ ì§ˆë¬¸
    gr.Markdown("### ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸:")
    gr.Examples(
        examples=[
            ["Classify the hair length in this image. Possible values: short, mid, long. Output only one word."],
            ["Describe the person's hair style, color, and texture in detail."],
            ["What is the hair length? Answer in one word: short, mid, or long."],
            ["Describe what you see in this image."],
            ["ì´ ì‚¬ëŒì˜ ë¨¸ë¦¬ ê¸¸ì´ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”. ê°€ëŠ¥í•œ ê°’: short, mid, long"],
        ],
        inputs=[question],
    )
    
    # ìë™ ì¶”ë¡  ì œì–´
    is_auto_running = gr.State(value=False)
    last_inference_time = gr.State(value=0)
    
    def start_auto_inference():
        """ìë™ ì¶”ë¡  ì‹œì‘"""
        # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¦‰ì‹œ ì²« ì¶”ë¡  ì‹œì‘
        return True, "â–¶ï¸ ì‹¤í–‰ ì¤‘ (3ì´ˆ ê°„ê²©)", gr.Timer(value=0.5, active=True), time.time() - INFERENCE_INTERVAL
    
    def stop_auto_inference():
        """ìë™ ì¶”ë¡  ì¤‘ì§€"""
        return False, "â¸ï¸ ë©ˆì¶¤", gr.Timer(value=0.5, active=False)
    
    def auto_inference_loop(image, question_text, is_running, last_time):
        """ìë™ ì¶”ë¡  ë£¨í”„ (3ì´ˆë§ˆë‹¤ ì‹¤í–‰)"""
        if not is_running:
            return gr.update(), gr.update(), last_time
        
        current_time = time.time()
        
        # ì´ë¯¸ì§€ ì—†ìœ¼ë©´ ê²½ê³  ë©”ì‹œì§€
        if image is None:
            return gr.update(), "âš ï¸ ì›¹ìº  ì´ë¯¸ì§€ë¥¼ ìº¡ì²˜í•´ì£¼ì„¸ìš”", last_time
        
        # 3ì´ˆ ê²½ê³¼ í™•ì¸
        if current_time - last_time >= INFERENCE_INTERVAL:
            result, status_msg = inference(image, question_text)
            return result, status_msg, current_time
        else:
            # ëŒ€ê¸° ì¤‘ ë‚¨ì€ ì‹œê°„ í‘œì‹œ
            remaining = INFERENCE_INTERVAL - (current_time - last_time)
            return gr.update(), f"â±ï¸ ë‹¤ìŒ ì¶”ë¡ ê¹Œì§€ {remaining:.1f}ì´ˆ", last_time
    
    # ìë™ ì¶”ë¡  íƒ€ì´ë¨¸
    timer = gr.Timer(value=0.5, active=False)
    
    # ì‹œì‘ ë²„íŠ¼
    start_btn.click(
        fn=start_auto_inference,
        inputs=[],
        outputs=[is_auto_running, auto_status, timer, last_inference_time]
    )
    
    # ì¤‘ì§€ ë²„íŠ¼
    stop_btn.click(
        fn=stop_auto_inference,
        inputs=[],
        outputs=[is_auto_running, auto_status, timer]
    )
    
    # íƒ€ì´ë¨¸ í‹±
    timer.tick(
        fn=auto_inference_loop,
        inputs=[webcam, question, is_auto_running, last_inference_time],
        outputs=[output, status, last_inference_time]
    )


if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸš€ Launching at http://0.0.0.0:7860")
    print("="*70 + "\n")
    
    demo.launch(
        server_name="0.0.0.0",
        server_port=8085,
        share=False,
        show_error=True
    )

